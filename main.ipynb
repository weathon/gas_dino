{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADS = 8\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.v2\n",
    "import torchvision.transforms.functional\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, mode=\"train\"):\n",
    "        self.mode = mode\n",
    "        if mode == \"train\":\n",
    "            self.datapath = \"/home/wg25r/fastdata/gasvid/train\"\n",
    "            self.images = [i for i in os.listdir(f\"{self.datapath}/masks\") if int(i.split(\"_\")[-1].split(\".\")[0]) > 500]\n",
    "        else: \n",
    "            self.datapath = \"/home/wg25r/fastdata/gasvid/val\"\n",
    "            self.images = [i for i in os.listdir(f\"{self.datapath}/masks\") if int(i.split(\"_\")[-1].split(\".\")[0]) > 500]\n",
    "\n",
    "        self.ignore_before = 20\n",
    "        self.ignore_after = 40 \n",
    "        self.space_trans = torchvision.transforms.v2.Compose([\n",
    "            torchvision.transforms.v2.RandomResizedCrop(448, scale=(0.6, 3)), \n",
    "            torchvision.transforms.v2.RandomHorizontalFlip(0.5),\n",
    "            torchvision.transforms.v2.RandomRotation(20), \n",
    "            torchvision.transforms.v2.RandomApply(\n",
    "                [torchvision.transforms.v2.ElasticTransform(alpha=50)], p=0.3\n",
    "            ),\n",
    "            torchvision.transforms.v2.RandomApply(\n",
    "                [torchvision.transforms.v2.RandomPerspective()], p=0.3\n",
    "            ),\n",
    "            torchvision.transforms.v2.RandomApply(\n",
    "                [torchvision.transforms.v2.RandomAffine(20,  scale=(0.5, 1.1))], p=0.3\n",
    "            ),\n",
    "        ])\n",
    "        self.color_trans = torchvision.transforms.v2.Compose([\n",
    "            torchvision.transforms.v2.RandomApply([torchvision.transforms.v2.GaussianBlur(3, sigma=(0.1, 2.0))], p=0.4)\n",
    "        ])\n",
    "\n",
    "    def __len__(self): \n",
    "        if self.mode == \"train\":\n",
    "            return int(len(self.images)) \n",
    "        else: \n",
    "            return int(len(self.images))\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        filename = self.images[idx]\n",
    "\n",
    "        current_frame = cv2.resize(cv2.imread(f\"{self.datapath}/in/{filename}\"), (512, 512))\n",
    "        long_bg = cv2.resize(cv2.imread(f\"{self.datapath}/long/{filename}\"), (512, 512))\n",
    "        short_bg = cv2.resize(cv2.imread(f\"{self.datapath}/short/{filename}\"), (512, 512)) \n",
    "        label_ = cv2.imread(f\"{self.datapath}/gt/{filename}\")\n",
    "        label = (label_ == 255) * 255.0\n",
    "        ROI =  (label_ != 85) * 255.0\n",
    "        label = cv2.resize(label, (512, 512))\n",
    "        ROI = cv2.resize(ROI, (512, 512))\n",
    "\n",
    "        current_frame = torch.tensor(current_frame).permute(2,0,1)\n",
    "        long_bg = torch.tensor(long_bg).permute(2,0,1)\n",
    "        short_bg = torch.tensor(short_bg).permute(2,0,1)\n",
    "        label = torch.tensor(label).permute(2,0,1)\n",
    "        X = torch.cat([current_frame, long_bg, short_bg], axis=0)\n",
    "        # print(X.shape)\n",
    "        \n",
    "        Y = label.max(0)[None,:,:] \n",
    "\n",
    "        if self.mode == \"train\":  \n",
    "            # X = self.color_trans(X) \n",
    "            YX = torch.cat((Y, X), axis=0) \n",
    "            YX = self.space_trans(YX)\n",
    "            Y = YX[:1]/255.0  \n",
    "            X = YX[1:]/255.0 \n",
    "            X += torch.randn(X.shape) * 0.005\n",
    "            X += torch.tensor(cv.resize(np.random.normal(0, 0.005, (10, 10)), X.shape[1:])).float()\n",
    "            X *= 1 + torch.randn(9)[:,None,None] * 0.005\n",
    "        else:\n",
    "            X = X/255.0\n",
    "            Y = Y/255.0 \n",
    "        Y = torchvision.transforms.functional.resize(Y, (448//4, 448//4))[0]\n",
    "        X[X<0]=0\n",
    "        return X, Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/wg25r/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    }
   ],
   "source": [
    "vits8 = torch.hub.load('facebookresearch/dino:main', 'dino_vits8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCA(nn.Module):\n",
    "    \"\"\"\n",
    "    Background-CurrentFrame Attention\n",
    "    \"\"\"\n",
    "    def __init__(self, dim=384):\n",
    "        super(BCA, self).__init__()\n",
    "        self.cross_attention = nn.MultiheadAttention(dim, HEADS, dropout=0.1, batch_first=True, kdim=dim, vdim=dim * 2)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "    \n",
    "    def forward(self, background, current_frame):\n",
    "        \"\"\"\n",
    "        background: torch.Tensor, shape (batch, L, dim)\n",
    "        current_frame: torch.Tensor, shape (batch, L, dim)\n",
    "        \"\"\"\n",
    "\n",
    "        attn_output, _ = self.cross_attention(query=current_frame, key=background, value=torch.concatenate([background, current_frame], dim=-1))\n",
    "        attn_output = self.norm1(attn_output + current_frame)\n",
    "        mlp_output = self.mlp(attn_output)\n",
    "        mlp_output = self.norm2(mlp_output + attn_output)\n",
    "        return mlp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCA_list(nn.Module):\n",
    "    \"\"\"\n",
    "    Background-CurrentFrame Attention\n",
    "    \"\"\"\n",
    "    def __init__(self, dim=384):\n",
    "        super(BCA, self).__init__()\n",
    "        self.cross_attention = nn.MultiheadAttention(dim, HEADS, dropout=0.1, batch_first=True, kdim=dim, vdim=dim * 2)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "    \n",
    "    def forward(self, long_background, short_background, toupimahuxikunduzikk current_frame):\n",
    "        \"\"\"\n",
    "        background: torch.Tensor, shape (batch, L, dim)\n",
    "        current_frame: torch.Tensor, shape (batch, L, dim)\n",
    "        \"\"\"\n",
    "\n",
    "        attn_output, _ = self.cross_attention(query=current_frame, key=background, value=torch.concatenate([background, current_frame], dim=-1))\n",
    "        attn_output = self.norm1(attn_output + current_frame)\n",
    "        mlp_output = self.mlp(attn_output)\n",
    "        mlp_output = self.norm2(mlp_output + attn_output)\n",
    "        return mlp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.bca_seq = nn.Sequential(*[BCA() for _ in range(4)])\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bicubic'),\n",
    "            nn.Conv2d(384, 1, 1)\n",
    "        ) \n",
    "        \n",
    "\n",
    "    def forward(self, bg, current_frame):\n",
    "        bg = self.backbone.get_intermediate_layers(bg)[0][:,1:,:]\n",
    "        current_frame = self.backbone.get_intermediate_layers(current_frame)[0][:,1:,:]\n",
    "        for bca in self.bca_seq:\n",
    "            current_frame = bca(bg, current_frame)\n",
    "        return self.decoder(current_frame.reshape(bg.shape[0], 448//8, 448//8, 384).permute(0,3,1,2))\n",
    "    \n",
    "\n",
    "\n",
    "# MyModel(vits8)(torch.randn(1, 3, 448, 448), torch.randn(1, 3, 448, 448)).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 14884 Val 77\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(MyDataset(\"train\"), batch_size=8, shuffle=True, num_workers=80, persistent_workers=True, prefetch_factor=3)\n",
    "val_dataloader = torch.utils.data.DataLoader(MyDataset(\"val\"), batch_size=8, shuffle=True, num_workers=80, persistent_workers=True, prefetch_factor=3)\n",
    "print(\"Train\", len(train_dataloader), \"Val\", len(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_loss(pred, target):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    assert pred.shape == target.shape\n",
    "    e = 1e-6\n",
    "    iou = ((pred * target).sum() + e) / (pred.sum() + target.sum() - (pred * target).sum() + e)\n",
    "    return 1 - iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.2.3) or chardet (5.2.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwguo6358\u001b[0m (\u001b[33m3dsmile\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/wg25r/gas_dino/wandb/run-20241009_205806-5dxhd6r7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/3dsmile/gas_dino/runs/5dxhd6r7' target=\"_blank\">efficient-water-7</a></strong> to <a href='https://wandb.ai/3dsmile/gas_dino' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/3dsmile/gas_dino' target=\"_blank\">https://wandb.ai/3dsmile/gas_dino</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/3dsmile/gas_dino/runs/5dxhd6r7' target=\"_blank\">https://wandb.ai/3dsmile/gas_dino/runs/5dxhd6r7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/3dsmile/gas_dino/runs/5dxhd6r7?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7d57278ccc70>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "mymodel = MyModel(vits8).cuda()\n",
    "optimizer = torch.optim.Adam(mymodel.parameters(), lr=3e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 500, eta_min=1e-6)\n",
    "wandb.init(config={\n",
    "    \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "    \"batch_size\": train_dataloader.batch_size,\n",
    "}, resume=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wg25r/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss 0.9872101785300614 Val iou tensor(0.0195, device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m pred \u001b[38;5;241m=\u001b[39m mymodel(X_val[:,\u001b[38;5;241m0\u001b[39m][:,\u001b[38;5;28;01mNone\u001b[39;00m,:,:]\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m , X_val[:,\u001b[38;5;241m1\u001b[39m][:,\u001b[38;5;28;01mNone\u001b[39;00m,:,:]\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, Y_val\u001b[38;5;241m.\u001b[39mcuda()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)) \n\u001b[0;32m---> 21\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m iou \u001b[38;5;241m=\u001b[39m (((pred \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m (Y_val\u001b[38;5;241m.\u001b[39mcuda()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-6\u001b[39m)\u001b[38;5;241m/\u001b[39m(((pred \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m|\u001b[39m ((Y_val\u001b[38;5;241m.\u001b[39mcuda()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)))\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-6\u001b[39m)\n\u001b[1;32m     23\u001b[0m pred \u001b[38;5;241m=\u001b[39m pred[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m448\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m448\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_fn = iou_loss\n",
    "epoch = 0\n",
    "while 1:\n",
    "    for i, (X, Y) in enumerate(train_dataloader):\n",
    "        if i%500 == 0:\n",
    "            scheduler.step() \n",
    "\n",
    "        if i%200==0:\n",
    "            with torch.no_grad():\n",
    "                mymodel.eval()\n",
    "                total_loss = 0\n",
    "                total_iou = 0\n",
    "                for X_val, Y_val in val_dataloader:\n",
    "                    X_val = X_val.cuda().float()\n",
    "                    Y_val = Y_val.cuda().float()\n",
    "\n",
    "                    pred = mymodel(X_val[:,0][:,None,:,:].repeat(1,3,1,1)\n",
    "                    , X_val[:,1][:,None,:,:].repeat(1,3,1,1)\n",
    "                    )\n",
    "                    loss = loss_fn(pred, Y_val.cuda().unsqueeze(1)) \n",
    "                    total_loss += loss.item()\n",
    "                    iou = (((pred > 0) & (Y_val.cuda().unsqueeze(1) > 0)).float().mean() + 1e-6)/(((pred > 0) | ((Y_val.cuda().unsqueeze(1) > 0))).float().mean() + 1e-6)\n",
    "                    pred = pred[0].reshape((448//4, 448//4))\n",
    "                    total_iou += iou.float()\n",
    "                total_iou /= len(val_dataloader) \n",
    "                if total_iou > 0.5:\n",
    "                    torch.save(mymodel.state_dict(), \"final.pth\") \n",
    "\n",
    "                total_loss /= len(val_dataloader) \n",
    "                wandb.log({\"val_iou\": total_iou, \"has_gas_ratio\":(Y_val.sum((1,2)) > 0).float().sum()/len(Y_val),\n",
    "                    \"real\": wandb.Image(Y_val[0].cpu().detach().numpy().reshape(448//4, 448//4)), \n",
    "                    \"pred\": wandb.Image(pred.cpu().detach().numpy()>0),\n",
    "                      \"X_val\": wandb.Image(X_val[0][0].cpu().detach().numpy()),\n",
    "                      \"X_BGS\": wandb.Image(X_val[0][1].cpu().detach().numpy()),\n",
    "                      \"val_loss\": total_loss}) \n",
    "                print(\"Val loss\", total_loss, \"Val iou\", total_iou)\n",
    "                mymodel.train()\n",
    "                epoch += 1\n",
    "\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        X = X.cuda().float() \n",
    "        Y = Y.cuda().float()\n",
    "        pred = mymodel(X[:,0][:,None,:,:].repeat(1,3,1,1), X[:,1][:,None,:,:].repeat(1,3,1,1))\n",
    "        # loss = torchvision.ops.sigmoid_focal_loss(pred, Y.cuda().unsqueeze(1), alpha=1/(labels == 1).sum(), gamma=10, reduction=\"mean\")\n",
    "        loss = loss_fn(pred, Y.cuda().unsqueeze(1))\n",
    "        acc = (pred > 0) == Y.cuda().unsqueeze(1)\n",
    "        # f1 = f1_score(Y.unsqueeze(1).cpu().detach().numpy().reshape(-1).astype(int), pred.cpu().detach().numpy().reshape(-1) > 0)\n",
    "        iou = (((pred > 0) & (Y.cuda().unsqueeze(1) > 0)).float().mean()  + 1e-6 )/(((pred > 0) | (Y.cuda().unsqueeze(1) > 0)) + 1e-6).float().mean()\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        wandb.log({\"loss\": loss.item(), \"acc\": acc.float().mean().item(), \"iou\": iou.float(), \"lr\": optimizer.param_groups[0][\"lr\"]}) # cannot do iou mean here otherwise it average non overlapping area\n",
    "        # same pred as image to wandb\n",
    "        if i % 2000 == 0:\n",
    "            torch.save(mymodel.state_dict(), \"model_ft.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 1, 224, 224]), torch.Size([8, 1, 112, 112]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape, Y_val.unsqueeze(1).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
